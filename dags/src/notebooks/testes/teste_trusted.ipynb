{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "     -------------------------------------- 310.8/310.8 MB 7.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ------------------------------------- 200.5/200.5 kB 12.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285411 sha256=d141f46ab0642cf57bea2ae4aeadfc75b897f3a9090dd67ae989c87751ecb60d\n",
      "  Stored in directory: c:\\users\\nicol\\appdata\\local\\pip\\cache\\wheels\\2b\\9a\\39\\d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285411 sha256=6c21ea42a95f3da6b674a9e1249f850a50e2c32131d523c9410e0039c8a6ff7d\n",
      "  Stored in directory: c:\\users\\nicol\\appdata\\local\\pip\\cache\\wheels\\2b\\9a\\39\\d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Carga_Trusted\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_trusted = '/PastaCompartilhadaHost/desafio-data-engineer/datalake/trusted/*/*/*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"mergeSchema\", \"true\").parquet(filepath_trusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+----------+-------------------+----------------------+-----------------+----------------------+\n",
      "|               pais|              estado| latitude| longitude|               data|quantidade_confirmados|quantidade_mortes|quantidade_recuperados|\n",
      "+-------------------+--------------------+---------+----------+-------------------+----------------------+-----------------+----------------------+\n",
      "|        Afghanistan|                  -1| 33.93911| 67.709953|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Albania|                  -1|  41.1533|   20.1683|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Algeria|                  -1|  28.0339|    1.6596|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Andorra|                  -1|  42.5063|    1.5218|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|             Angola|                  -1| -11.2027|   17.8739|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|Antigua and Barbuda|                  -1|  17.0608|  -61.7964|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Argentina|                  -1| -38.4161|  -63.6167|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Armenia|                  -1|  40.0691|   45.0382|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|  Northern Territory| -12.4634|  130.8456|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|          Queensland| -27.4698|  153.0251|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|     South Australia| -34.9285|  138.6007|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|   Western Australia| -31.9505|  115.8605|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|     New South Wales| -33.8688|  151.2093|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|            Tasmania| -42.8821|  147.3272|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|            Victoria| -37.8136|  144.9631|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|          Australia|Australian Capita...| -35.4735|  149.0124|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Austria|                  -1|  47.5162|   14.5501|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|         Azerbaijan|                  -1|  40.1431|   47.5769|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Bahamas|                  -1|25.025885|-78.035889|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "|            Bahrain|                  -1|  26.0275|     50.55|2020-01-22 00:00:00|                     0|                0|                     0|\n",
      "+-------------------+--------------------+---------+----------+-------------------+----------------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"data\",\"pais\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, to_timestamp, sum, avg \n",
    "from pyspark.sql.functions import array, explode, struct, lit, col\n",
    "from airflow.decorators import task\n",
    "import os  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_input = '/PastaCompartilhadaHost/desafio-data-engineer/datalake/raw/covid19'\n",
    "filepath_output = '/PastaCompartilhadaHost/desafio-data-engineer/datalake/trusted'\n",
    "\n",
    "\n",
    "# Para os arquivos de input, geera os respectivos dfs\n",
    "path_confirmados = os.path.join(filepath_input,'time_series_covid19_confirmed_global.csv')\n",
    "df_confirmados = spark.read.csv(path_confirmados, header=True, inferSchema=True)\n",
    "path_mortes = os.path.join(filepath_input,'time_series_covid19_deaths_global.csv')\n",
    "df_mortes = spark.read.csv(path_mortes, header=True, inferSchema=True)\n",
    "path_recuperados = os.path.join(filepath_input,'time_series_covid19_recovered_global.csv')\n",
    "df_recuperados = spark.read.csv(path_recuperados, header=True, inferSchema=True)\n",
    "\n",
    "# Ajusta cada df transformando colunas de datas em linhas\n",
    "# Para que seja possível join mais a frente sem cartesiano, necessário preencher valores nulos\n",
    "# Tentou-se estratégias sem preenchimento de valores, como utilizando eqNullSafe com full join, sem sucesso\n",
    "df_confirmados_aj = ajusta_df(df_confirmados, \\\n",
    "            [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], \\\n",
    "            \"data_t\", \\\n",
    "            \"quantidade_confirmados\") \\\n",
    "            .na.fill(value=\"-1\",subset=[\"Province/State\"])\\\n",
    "            .na.fill(value=-1,subset=[\"Lat\", \"Long\"])\n",
    "                \n",
    "df_mortes_aj = ajusta_df(df_mortes, \\\n",
    "                [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], \\\n",
    "                \"data_t\", \\\n",
    "                \"quantidade_mortes\") \\\n",
    "                .na.fill(value=\"-1\",subset=[\"Province/State\"])\\\n",
    "                .na.fill(value=-1,subset=[\"Lat\", \"Long\"])\n",
    "df_recuperados_aj = ajusta_df(df_recuperados, \\\n",
    "                [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], \\\n",
    "                \"data_t\", \\\n",
    "                \"quantidade_recuperados\") \\\n",
    "                .na.fill(value=\"-1\",subset=[\"Province/State\"])\\\n",
    "                .na.fill(value=-1,subset=[\"Lat\", \"Long\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------+------------------+------------------+------+----------------------+\n",
      "|summary|Province/State|Country/Region|               Lat|              Long|data_t|quantidade_confirmados|\n",
      "+-------+--------------+--------------+------------------+------------------+------+----------------------+\n",
      "|  count|        131175|        131175|            131175|            131175|131175|                131175|\n",
      "|   mean|          -1.0|          null|20.378187454548296|22.853393141819883|  null|    178641.85499523536|\n",
      "| stddev|           0.0|          null| 25.12360910363382| 73.22238037277491|  null|    1223316.1245162636|\n",
      "|    min|            -1|   Afghanistan|          -51.7963|         -178.1165|1/1/21|                     0|\n",
      "|    25%|          -1.0|          null|            4.5709|          -19.0208|  null|                    81|\n",
      "|    50%|          -1.0|          null|         21.521757|         20.902977|  null|                  1291|\n",
      "|    75%|          -1.0|          null|           41.1129|             84.25|  null|                 25737|\n",
      "|    max|      Zhejiang|      Zimbabwe|           71.7069|           178.065|9/9/20|              32814784|\n",
      "+-------+--------------+--------------+------------------+------------------+------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_confirmados_aj.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------+------------------+------------------+------+------------------+\n",
      "|summary|Province/State|Country/Region|               Lat|              Long|data_t| quantidade_mortes|\n",
      "+-------+--------------+--------------+------------------+------------------+------+------------------+\n",
      "|  count|        131175|        131175|            131175|            131175|131175|            131175|\n",
      "|   mean|          -1.0|          null|20.378187454548296|22.853393141819883|  null| 4311.777091671432|\n",
      "| stddev|           0.0|          null| 25.12360910363382| 73.22238037277491|  null|24884.883496583665|\n",
      "|    min|            -1|   Afghanistan|          -51.7963|         -178.1165|1/1/21|                 0|\n",
      "|    25%|          -1.0|          null|            4.5709|          -19.0208|  null|                 1|\n",
      "|    50%|          -1.0|          null|         21.521757|         20.902977|  null|                20|\n",
      "|    75%|          -1.0|          null|           41.1129|             84.25|  null|               449|\n",
      "|    max|      Zhejiang|      Zimbabwe|           71.7069|           178.065|9/9/20|            583685|\n",
      "+-------+--------------+--------------+------------------+------------------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_mortes_aj.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------+------------------+-----------------+------+----------------------+\n",
      "|summary|Province/State|Country/Region|               Lat|             Long|data_t|quantidade_recuperados|\n",
      "+-------+--------------+--------------+------------------+-----------------+------+----------------------+\n",
      "|  count|        124020|        124020|            124020|           124020|124020|                124020|\n",
      "|   mean|          -1.0|          null|19.025420719234248|28.38237537693291|  null|    110515.60972423802|\n",
      "| stddev|           0.0|          null|24.577543028073567| 70.6865522696299|  null|     657530.0087847405|\n",
      "|    min|            -1|   Afghanistan|          -51.7963|        -178.1165|1/1/21|                     0|\n",
      "|    25%|          -1.0|          null|            4.5353|        -9.429499|  null|                    29|\n",
      "|    50%|          -1.0|          null|           19.3133|          23.8813|  null|                   923|\n",
      "|    75%|          -1.0|          null|           38.9637|          90.3563|  null|                 15697|\n",
      "|    max|      Zhejiang|      Zimbabwe|           71.7069|          178.065|9/9/20|              19734823|\n",
      "+-------+--------------+--------------+------------------+-----------------+------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_recuperados_aj.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recuperados_aj.withColumnRenamed(\"Country/Region\",\"pais\") \\\n",
    ".withColumnRenamed(\"Province/State\",\"estado\") \\\n",
    ".withColumnRenamed(\"Lat\",\"latitude\") \\\n",
    ".withColumnRenamed(\"Long\",\"longitude\").createOrReplaceTempView('rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmados_aj.withColumnRenamed(\"Country/Region\",\"pais\") \\\n",
    ".withColumnRenamed(\"Province/State\",\"estado\") \\\n",
    ".withColumnRenamed(\"Lat\",\"latitude\") \\\n",
    ".withColumnRenamed(\"Long\",\"longitude\").createOrReplaceTempView('conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2862"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select rec.* from rec left join conf on rec.estado == conf.estado and rec.pais == conf.pais and rec.data_t == conf.data_t and rec.latitude == conf.latitude and rec.longitude == conf.longitude where conf.pais is null and conf.estado is null and conf.data_t is null and conf.latitude is null and conf.longitude is null\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+---------+-------+----------------------+\n",
      "|estado|  pais|latitude|longitude| data_t|quantidade_recuperados|\n",
      "+------+------+--------+---------+-------+----------------------+\n",
      "|    -1|Canada| 56.1304|-106.3468|1/22/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/23/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/24/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/25/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/26/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/27/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/28/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/29/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/30/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/31/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/1/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/2/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/3/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/4/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/5/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/6/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/7/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/8/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/9/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|2/10/20|                     0|\n",
      "+------+------+--------+---------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select rec.* from rec left join conf on rec.estado == conf.estado and rec.pais == conf.pais and rec.data_t == conf.data_t and rec.latitude == conf.latitude and rec.longitude == conf.longitude where conf.pais is null and conf.estado is null and conf.data_t is null and conf.latitude is null and conf.longitude is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+---------+------+----------------------+\n",
      "|estado|pais|latitude|longitude|data_t|quantidade_confirmados|\n",
      "+------+----+--------+---------+------+----------------------+\n",
      "+------+----+--------+---------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from conf where pais = 'Canada' and estado ='-1' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+---------+-------+----------------------+\n",
      "|estado|  pais|latitude|longitude| data_t|quantidade_recuperados|\n",
      "+------+------+--------+---------+-------+----------------------+\n",
      "|    -1|Canada| 56.1304|-106.3468|1/22/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/23/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/24/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/25/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/26/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/27/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/28/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/29/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/30/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|1/31/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/1/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/2/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/3/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/4/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/5/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/6/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/7/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/8/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468| 2/9/20|                     0|\n",
      "|    -1|Canada| 56.1304|-106.3468|2/10/20|                     0|\n",
      "+------+------+--------+---------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from rec where pais = 'Canada' and estado ='-1' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unifica os dataframes\n",
    "merged_df = df_confirmados_aj.join(df_mortes_aj, \\\n",
    "                            [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\", \"data_t\"], \\\n",
    "                                how=\"full\") \\\n",
    "                            .join(df_recuperados_aj, \\\n",
    "                                [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\", \"data_t\"], \\\n",
    "                                how=\"full\")\n",
    "\n",
    "# Gera df com colunas e estruturas desejadas\n",
    "merged_df = merged_df.select(\"*\") \\\n",
    ".withColumnRenamed(\"Country/Region\",\"pais\") \\\n",
    ".withColumnRenamed(\"Province/State\",\"estado\") \\\n",
    ".withColumnRenamed(\"Lat\",\"latitude\") \\\n",
    ".withColumnRenamed(\"Long\",\"longitude\") \\\n",
    ".withColumn(\"data\", to_timestamp(col(\"data_t\"),\"M/d/yy\")) \\\n",
    ".drop(\"data_t\") \\\n",
    ".withColumn(\"ano\",year(col(\"data\"))) \\\n",
    ".withColumn(\"mes\",month(col(\"data\")))\n",
    "\n",
    "# Gera df com ordem de colunas desejada e campos de qtd com tipagem desejada\n",
    "merged_df = merged_df.selectExpr(\"pais\", \\\n",
    "                        \"estado\", \\\n",
    "                        \"latitude\", \\\n",
    "                        \"longitude\", \\\n",
    "                        \"data\", \\\n",
    "                        \"cast(quantidade_confirmados as long) quantidade_confirmados\", \\\n",
    "                        \"cast(quantidade_mortes as long) quantidade_mortes\", \\\n",
    "                        \"cast(quantidade_recuperados as long) quantidade_recuperados\", \\\n",
    "                        \"ano\",\n",
    "                        \"mes\"\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView('merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|  pais|estado|latitude|longitude|               data|quantidade_confirmados|quantidade_mortes|quantidade_recuperados| ano|mes|\n",
      "+------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-01 00:00:00|                  null|             null|                494437|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-10 00:00:00|                  null|             null|                565049|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-11 00:00:00|                  null|             null|                575152|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-13 00:00:00|                  null|             null|                591131|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-14 00:00:00|                  null|             null|                599753|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-15 00:00:00|                  null|             null|                608322|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-17 00:00:00|                  null|             null|                621857|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-18 00:00:00|                  null|             null|                629547|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-02 00:00:00|                  null|             null|                497492|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-20 00:00:00|                  null|             null|                644967|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2020-01-23 00:00:00|                  null|             null|                     0|2020|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-23 00:00:00|                  null|             null|                663552|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2020-01-24 00:00:00|                  null|             null|                     0|2020|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-24 00:00:00|                  null|             null|                670938|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-25 00:00:00|                  null|             null|                677636|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-27 00:00:00|                  null|             null|                690369|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-28 00:00:00|                  null|             null|                695578|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2020-01-29 00:00:00|                  null|             null|                     0|2020|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-29 00:00:00|                  null|             null|                701306|2021|  1|\n",
      "|Canada|    -1| 56.1304|-106.3468|2021-01-03 00:00:00|                  null|             null|                510595|2021|  1|\n",
      "+------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/23 13:46:16 WARN DAGScheduler: Broadcasting large task binary with size 1146.5 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from merged where pais=\"Canada\" and estado = -1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|  pais| estado|latitude|longitude|               data|quantidade_confirmados|quantidade_mortes|quantidade_recuperados| ano|mes|\n",
      "+------+-------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-10 00:00:00|                111452|             1284|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-12 00:00:00|                112743|             1345|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-14 00:00:00|                114585|             1389|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-15 00:00:00|                115370|             1402|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-17 00:00:00|                116837|             1436|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-18 00:00:00|                117311|             1447|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-19 00:00:00|                117767|             1463|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-02 00:00:00|                100428|             1046|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-20 00:00:00|                118436|             1484|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-21 00:00:00|                119114|             1500|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-22 00:00:00|                119757|             1512|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2020-01-24 00:00:00|                     0|                0|                  null|2020|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2020-01-27 00:00:00|                     0|                0|                  null|2020|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-28 00:00:00|                122821|             1606|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2020-01-29 00:00:00|                     0|                0|                  null|2020|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-03 00:00:00|                104228|             1046|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2020-01-30 00:00:00|                     0|                0|                  null|2020|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2020-01-31 00:00:00|                     0|                0|                  null|2020|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-06 00:00:00|                107501|             1193|                  null|2021|  1|\n",
      "|Canada|Alberta| 53.9333|-116.5765|2021-01-07 00:00:00|                108469|             1217|                  null|2021|  1|\n",
      "+------+-------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/23 13:47:16 WARN DAGScheduler: Broadcasting large task binary with size 1146.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from merged where pais=\"Canada\" and estado <> \"-1\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-----------------+-----------------+----------------------+-----------------+----------------------+\n",
      "|summary|       pais|  estado|         latitude|        longitude|quantidade_confirmados|quantidade_mortes|quantidade_recuperados|\n",
      "+-------+-----------+--------+-----------------+-----------------+----------------------+-----------------+----------------------+\n",
      "|  count|     134037|  134037|           134037|           134037|                131175|           131175|                124020|\n",
      "|   mean|       null|    -1.0|20.42409911031997|23.51284133096057|    178641.85499523536|4311.777091671432|    110515.60972423802|\n",
      "| stddev|       null|     0.0| 25.1637945552342|73.51623563186946|    1223316.1245162634|24884.88349658362|      657530.008784745|\n",
      "|    min|Afghanistan|      -1|         -51.7963|        -178.1165|                     0|                0|                     0|\n",
      "|    25%|       null|    -1.0|           4.5709|         -15.3101|                    81|                1|                    29|\n",
      "|    50%|       null|    -1.0|           21.694|          21.0059|                  1291|               20|                   923|\n",
      "|    75%|       null|    -1.0|        40.463667|          85.2401|                 25717|              449|                 15688|\n",
      "|    max|   Zimbabwe|Zhejiang|          71.7069|          178.065|              32814784|           583685|              19734823|\n",
      "+-------+-----------+--------+-----------------+-----------------+----------------------+-----------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, avg, days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted = df.groupBy(col(\"pais\"), \\\n",
    "            col(\"data\"), \\\n",
    "            col(\"ano\")) \\\n",
    "    .agg(sum(col(\"quantidade_confirmados\")).alias(\"soma_confirmados\"), \\\n",
    "         sum(col(\"quantidade_mortes\")).alias(\"soma_mortes\"), \\\n",
    "         sum(col(\"quantidade_recuperados\")).alias(\"soma_recuperados\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create window by casting timestamp to long (number of seconds)\n",
    "w = (Window.orderBy(col(\"data\")).partitionBy(col(\"pais\")).rowsBetween(-7, 0))\n",
    "\n",
    "df_trusted = df_trusted.withColumn(\"media_movel_confirmados\",avg(col(\"soma_confirmados\")).over(w)) \\\n",
    "    .withColumn(\"media_movel_mortes\",avg(col(\"soma_mortes\")).over(w)) \\\n",
    "    .withColumn(\"media_movel_recuperados\",avg(col(\"soma_recuperados\")).over(w)) \\\n",
    "    .select(col(\"pais\"), col(\"data\"), col(\"media_movel_confirmados\"), col(\"media_movel_mortes\"), col(\"media_movel_recuperados\"), col(\"ano\"))\n",
    "#.filter(col(\"media_confirmados\")>0) \\ \n",
    "#type(df_trusted)\n",
    "#df_trusted.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "|       pais|               data|media_movel_confirmados|media_movel_mortes|media_movel_recuperados| ano|\n",
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "|Afghanistan|2020-01-22 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-23 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-24 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-25 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-26 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-27 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-28 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-29 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-30 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-01-31 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-01 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-02 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-03 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-04 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-05 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-06 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-07 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-08 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-09 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "|Afghanistan|2020-02-10 00:00:00|                    0.0|               0.0|                    0.0|2020|\n",
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trusted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted.write.csv('/opt/airflow/dags/desafio-data-engineer/datalake/refined/teste',header=True,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('/opt/airflow/dags/desafio-data-engineer/datalake/refined/teste_leitura',header=True,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
